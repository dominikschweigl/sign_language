{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e17303",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe8515d8e0eb388ce79930ea7b6c63cb",
     "grade": false,
     "grade_id": "cell-14bdc41e163110b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Sign Language Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380380d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "340354f8bc0f0c53961e7f6762bd47e8",
     "grade": false,
     "grade_id": "cell-51db7564f748aef7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The Sign Language Dataset consists of 9680 grayscale images of hand signs for the digits 0-9 and the alphabets a-z. Thus, this is a multiclass classification problem with 36 classes. Your task is to build a machine learning model that can accurately classify images from this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87790d5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfd3766dc129d20a384b6fe374f898ba",
     "grade": false,
     "grade_id": "cell-e4af33c6fde73887",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Loading the dataset\n",
    "\n",
    "You **do not** need to upload any data. Both the visible training dataset and the hidden test dataset are already available on the Jupyter hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf043b71",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef93277d6faf4a26d52b648647386063",
     "grade": false,
     "grade_id": "cell-8c7257ef51480021",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4df5ed83",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a02a22a7620265e33abf343ddd5473e7",
     "grade": false,
     "grade_id": "cell-636bfe55501bec94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Setting the path of the training dataset (that was already provided to you)\n",
    "\n",
    "running_local = True if os.getenv('JUPYTERHUB_USER') is None else False\n",
    "DATASET_PATH = \".\"\n",
    "\n",
    "# Set the location of the dataset\n",
    "if running_local:\n",
    "    # If running on your local machine, the sign_lang_train folder's path should be specified here\n",
    "    local_path = \"./data/sign_lang_train\"\n",
    "    if os.path.exists(local_path):\n",
    "        DATASET_PATH = local_path\n",
    "else:\n",
    "    # If running on the Jupyter hub, this data folder is already available\n",
    "    # You DO NOT need to upload the data!\n",
    "    DATASET_PATH = \"/data/mlproject22/sign_lang_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba210aa6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80e80b0135d964a577c59224e58423d4",
     "grade": false,
     "grade_id": "cell-8532dde78b48d38e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Utility function\n",
    "\n",
    "def read_csv(csv_file):\n",
    "    with open(csv_file, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d7052",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc0334c0cfa0645319a201d00563c638",
     "grade": false,
     "grade_id": "cell-f6ce53d70b7a4b20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data Loading using PyTorch\n",
    "\n",
    "For creating and training your model, you can work with any machine learning library of your choice. \n",
    "\n",
    "If you choose to work with [PyTorch](https://pytorch.org/), you will need to create your own [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class for loading the data. This is provided below. See [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) for a nice example of how to create a custom data loading pipeline in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4dace",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d630842cb12330f47d985a7d0f22823f",
     "grade": false,
     "grade_id": "cell-0e305bc0958e0408",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, utils, io\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "\n",
    "from string import ascii_lowercase\n",
    "\n",
    "class SignLangDataset(Dataset):\n",
    "    \"\"\"Sign language dataset\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = read_csv(os.path.join(root_dir,csv_file))\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # List of class names in order\n",
    "        self.class_names = list(map(str, list(range(10)))) + list(ascii_lowercase)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Calculates the length of the dataset-\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns one sample (dict consisting of an image and its label)\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Read the image and labels\n",
    "        image_path = os.path.join(self.root_dir, self.data[idx][1])\n",
    "        \n",
    "        # Read image as a grayscale PIL image\n",
    "        image = Image.open(image_path).convert(\"L\")  # 'L' = grayscale mode\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # The label is the index of the class name in the list ['0','1',...,'9','a','b',...'z']\n",
    "        # because we should have integer labels in the range 0-35 (for 36 classes)\n",
    "        label = self.class_names.index(self.data[idx][0])\n",
    "        \n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def show_sample(self, idx: int) -> None:\n",
    "        \"\"\"\n",
    "        Displays a grayscale image with its corresponding class label (0-35).\n",
    "        \n",
    "        Args:\n",
    "            image (numpy.ndarray): The image to display. Shape should be (1, H, W) or (H, W).\n",
    "            label (int): Integer label in range 0-35.\n",
    "        \"\"\"\n",
    "\n",
    "        sample = self[idx]\n",
    "        image = sample[\"image\"]\n",
    "        label_index = sample[\"label\"]\n",
    "        label = self.class_names[label_index]\n",
    "\n",
    "        # Flatten channel dimension if needed\n",
    "        if image.ndim == 3 and image.shape[0] == 1:\n",
    "            image = image[0] \n",
    "        \n",
    "        plt.imshow(image, cmap=\"gray\")\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "# Define label mapping\n",
    "class_names = list(map(str, range(10))) + list(ascii_lowercase)\n",
    "\n",
    "# Transform Images to 128x128\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                   # Converts HxW numpy → CxHxW tensor\n",
    "    transforms.Resize((128, 128)),           # Resize to 128x128\n",
    "])\n",
    "\n",
    "sign_dataset = SignLangDataset(\"labels.csv\", os.path.join(DATASET_PATH, \"images\"), transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6000a3d",
   "metadata": {},
   "source": [
    "### Test Train Data Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8f75f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split sizes\n",
    "val_size = int(0.2 * len(sign_dataset))\n",
    "train_size = len(sign_dataset) - val_size\n",
    "\n",
    "# Split the dataset\n",
    "generator = torch.Generator().manual_seed(41) # for reproducability\n",
    "train_dataset, val_dataset = random_split(sign_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6212f",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfc8738b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device being used: cuda\n",
      "GPU name: NVIDIA GeForce GTX 1060 6GB\n",
      "<module 'torch.cuda' from 'c:\\\\Users\\\\schwe\\\\Documents\\\\Code\\\\ml_project\\\\.venv\\\\Lib\\\\site-packages\\\\torch\\\\cuda\\\\__init__.py'>\n",
      "PyTorch version: 2.1.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU name: NVIDIA GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SignLangCNN(nn.Module):\n",
    "    def __init__(self, num_classes=36):\n",
    "        super(SignLangCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)   # [B, 32, 128, 128]\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)                           # [B, 32, 64, 64]\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # [B, 64, 64, 64]\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)                           # [B, 64, 32, 32]\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # [B, 128, 32, 32]\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)                           # [B, 128, 16, 16]\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 256)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))  # [B, 32, 64, 64]\n",
    "        x = self.pool2(F.relu(self.conv2(x)))  # [B, 64, 32, 32]\n",
    "        x = self.pool3(F.relu(self.conv3(x)))  # [B, 128, 16, 16]\n",
    "        x = x.view(x.size(0), -1)              # flatten\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)                        # logits\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SignLangCNN(num_classes=36)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device being used:\", device)\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "print(torch.cuda) \n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6532219",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd172be1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m      4\u001b[39m     model.train()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\schwe\\Documents\\Code\\ml_project\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    633\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\schwe\\Documents\\Code\\ml_project\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    673\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    675\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    676\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\schwe\\Documents\\Code\\ml_project\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     51\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\schwe\\Documents\\Code\\ml_project\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\schwe\\Documents\\Code\\ml_project\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mSignLangDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     42\u001b[39m image = Image.open(image_path).convert(\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# 'L' = grayscale mode\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# The label is the index of the class name in the list ['0','1',...,'9','a','b',...'z']\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# because we should have integer labels in the range 0-35 (for 36 classes)\u001b[39;00m\n\u001b[32m     49\u001b[39m label = \u001b[38;5;28mself\u001b[39m.class_names.index(\u001b[38;5;28mself\u001b[39m.data[idx][\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\schwe\\Documents\\Code\\ml_project\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\schwe\\Documents\\Code\\ml_project\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\schwe\\Documents\\Code\\ml_project\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:166\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[32m    165\u001b[39m mode_to_nptype = {\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m: np.int32, \u001b[33m\"\u001b[39m\u001b[33mI;16\u001b[39m\u001b[33m\"\u001b[39m: np.int16, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m: np.float32}\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m img = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pic.mode == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    169\u001b[39m     img = \u001b[32m255\u001b[39m * img\n",
      "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64a601f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 3.10%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(images)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "val_acc = evaluate_accuracy(model, val_loader, device)\n",
    "print(f\"Validation Accuracy: {val_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76dbb6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 99.38%\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"sign_lang_model.pt\")\n",
    "\n",
    "val_acc = evaluate_accuracy(model, train_loader, device)\n",
    "print(f\"Validation Accuracy: {val_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28d4cb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_per_class_accuracy(model, dataloader, device, num_classes=36, class_names=None):\n",
    "    \"\"\"\n",
    "    Evaluates and plots per-class accuracy for a classification model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        dataloader: DataLoader for validation/test set.\n",
    "        device: 'cuda' or 'cpu'.\n",
    "        num_classes: Total number of classes.\n",
    "        class_names: Optional list of class labels. If None, uses ['0'-'9'] + ['a'-'z'].\n",
    "        show_worst_n: Number of lowest-performing classes to print.\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = list(map(str, range(10))) + list(ascii_lowercase)\n",
    "\n",
    "    model.eval()\n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            for label, pred in zip(labels, preds):\n",
    "                class_total[label] += 1\n",
    "                if label == pred:\n",
    "                    class_correct[label] += 1\n",
    "\n",
    "    per_class_accuracy = [correct / total if total != 0 else 0.0\n",
    "                          for correct, total in zip(class_correct, class_total)]\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.bar(class_names, per_class_accuracy)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Per-Class Accuracy\")\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()\n",
    "\n",
    "plot_per_class_accuracy(model, val_loader, device, 36, sign_dataset.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fd237d",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:  # or test_loader\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(36)))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, num_classes=36, normalize=False, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using matplotlib.\n",
    "    \n",
    "    Args:\n",
    "        y_true (list or array): Ground truth labels.\n",
    "        y_pred (list or array): Predicted labels.\n",
    "        num_classes (int): Number of classes.\n",
    "        normalize (bool): If True, normalize rows to show percentages.\n",
    "        cmap: Colormap for visualization.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "        cm = np.nan_to_num(cm)  # replace NaNs from division by zero\n",
    "\n",
    "    class_names = list(map(str, range(10))) + list(ascii_lowercase)\n",
    "\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, class_names, rotation=90)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Annotate cells\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd995bd8",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def k_fold_cross_validation(k_folds, dataset, model, epochs):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"\\n🔁 Fold {fold + 1}/{k_folds}\")\n",
    "        \n",
    "        # Create subset samplers\n",
    "        train_subset = Subset(dataset, train_ids)\n",
    "        val_subset = Subset(dataset, val_ids)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for batch in train_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluate\n",
    "        val_acc = evaluate_accuracy(model, val_loader, device)\n",
    "        print(f\"✅ Fold {fold + 1} Accuracy: {val_acc:.2%}\")\n",
    "        results[fold] = val_acc\n",
    "\n",
    "    average_acc = sum(results.values()) / k_folds\n",
    "    return average_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ec082",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f03a81c186cc5531728e6c012d0efd57",
     "grade": false,
     "grade_id": "cell-a177a28ccf1ee8bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Prediction Stub\n",
    "\n",
    "You will need to provide a function that can be used to make predictions using your final trained model. \n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "1. The name of your prediction function must be `leader_board_predict_fn`\n",
    "2. Your prediction function should be able take as input a 4-D numpy array of shape [batch_size,1,128,128] and produce predictions in the form of a 1-D numpy array of shape [batch_size,]. \n",
    "3. Predictions for each image should be an integer in the range 0-35, that is `0` for the digit $0$, `1` for the digit $1$, .... , `9` for the digit $9$, `10` for the letter $a$, `11` for the letter $b$, ..., `35` for the letter $z$.\n",
    "4. Your prediction function should internally load your trained model and take care of any data transformations that you need.\n",
    "\n",
    "Below we provide an implementation of the `leader_board_predict_fn` function, in which we show how a trained model can be loaded (from the weights saved on the disk) for making predictions. This example is for PyTorch, but you are free to use any framework of your choice for your model. The only requirement is that this function should accept a numpy array (with the proper shape) as the input and should produce a numpy array (with the proper shape) as the output. What you do internally is up to you.\n",
    "\n",
    "Note that the model that we load here is not properly trained and so its performance is very bad. This example is only for showing you how a model can be loaded in PyTorch and how predictions can be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8f100",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23c72c17b1ca862857647d412ae648ff",
     "grade": false,
     "grade_id": "cell-81520c1b1e481ca2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def leader_board_predict_fn(input_batch):\n",
    "    \"\"\"\n",
    "    Function for making predictions using your trained model.\n",
    "    \n",
    "    Args:\n",
    "        input_batch (numpy array): Input images (4D array of shape \n",
    "                                   [batch_size, 1, 128, 128])\n",
    "        \n",
    "    Returns:\n",
    "        output (numpy array): Predictions of the your trained model \n",
    "                             (1D array of int (0-35) of shape [batch_size, ])\n",
    "    \"\"\"\n",
    "    prediction = None\n",
    "    \n",
    "    batch_size, channels, height, width = input_batch.shape\n",
    "    \n",
    "       \n",
    "    ### BEGIN EXAMPLE \n",
    "    \n",
    "    # Load the network definition\n",
    "    from dummy_network import DummyNetwork\n",
    "    \n",
    "    # Instantiate the network and set the data type\n",
    "    dummy_network = DummyNetwork().float()\n",
    "    \n",
    "    # Load the saved weights from the disk\n",
    "    dummy_network.load_state_dict(torch.load(\"dummy_weights.pt\"))\n",
    "    \n",
    "    # Set the network to evaluation mode\n",
    "    dummy_network.eval()\n",
    "    \n",
    "    # VERY IMPORTANT\n",
    "    # Convert the input batch to a torch Tensor and set\n",
    "    # the data type to the same type as the network\n",
    "    input_batch = torch.from_numpy(input_batch).float()\n",
    "       \n",
    "    # A forward pass with the input batch produces a batch of logits\n",
    "    # In the network that we use here, Softmax is not applied to the output\n",
    "    # This may be different for your network.\n",
    "    logits = dummy_network(input_batch)\n",
    "    \n",
    "    # Final classification predictions are taken by taking an argmax over the logits\n",
    "    # The prediction is converted to a numpy array\n",
    "    prediction = torch.argmax(logits, dim=1).numpy()\n",
    "\n",
    "    ### END EXAMPLE \n",
    "\n",
    "    # Replace the entire section between ### BEGIN EXAMPLE and ### END EXAMPLE \n",
    "    # with your implementation\n",
    "    \n",
    "    # YOUR CODE HERE (please remove 'raise NotImplementedError()')\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    assert prediction is not None, \"Prediction cannot be None\"\n",
    "    assert isinstance(prediction, np.ndarray), \"Prediction must be a numpy array\"\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6fd33",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2820681a6fc16112534b10dfbeb2347b",
     "grade": false,
     "grade_id": "cell-4ac44e8d4bbf43c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Your final model will be evaluated on a hidden test set containing images similar to the dataset that you are provided with.\n",
    "\n",
    "For evaluating the performance of your model, we will use the normalized [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) metric from sklearn. This is simply the percentage of correct predictions that your model makes for all the images of the hidden test set. Hence, if all the predictions are correct, the score is 1.0 and if all predictions are incorrect, the score is 0.0. We will use the sklearn metric so that the accuracy function is agnostic to the machine learning framework you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5106fc5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "663b19833a6fe579d5d735111918221b",
     "grade": false,
     "grade_id": "cell-f6e73b401749aa78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "  \n",
    "def accuracy(dataset_path, max_batches=30):\n",
    "    \"\"\"\n",
    "    Calculates the average prediction accuracy.\n",
    "    \n",
    "    IMPORTANT\n",
    "    =========\n",
    "    In this function, we use PyTorch only for loading the data. When your `leader_board_predict_fn`\n",
    "    function is called, we pass the arguments to it as numpy arrays. The output of `leader_board_predict_fn`\n",
    "    is also expected to be a numpy array. So, as long as your `leader_board_predict_fn` function takes\n",
    "    numpy arrays as input and produces numpy arrays as output (with the proper shapes), it does not\n",
    "    matter what framework you used for training your network or for producing your predictions.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Path of the dataset directory\n",
    "        \n",
    "    Returns:\n",
    "        accuracy (float): Average accuracy score over all images (float in the range 0.0-1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a Dataset object\n",
    "    sign_lang_dataset = SignLangDataset(csv_file=\"labels.csv\", root_dir=dataset_path)\n",
    "\n",
    "    # Create a Dataloader\n",
    "    sign_lang_dataloader = DataLoader(sign_lang_dataset, \n",
    "                                      batch_size=64,\n",
    "                                      shuffle=True, \n",
    "                                      drop_last=True,\n",
    "                                      num_workers=0)\n",
    "    \n",
    "    # Calculate accuracy for each batch\n",
    "    accuracies = list()\n",
    "    for batch_idx, sample in enumerate(sign_lang_dataloader):\n",
    "        x = sample[\"image\"].numpy()\n",
    "        y = sample[\"label\"].numpy()\n",
    "        prediction = leader_board_predict_fn(x)\n",
    "        accuracies.append(accuracy_score(y, prediction, normalize=True))\n",
    "        \n",
    "        # We will consider only the first 30 batches\n",
    "        if batch_idx == (max_batches - 1):\n",
    "            break\n",
    "\n",
    "    assert len(accuracies) == max_batches\n",
    "    \n",
    "    # Return the average accuracy\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e4307",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb526476f67d9f610c05c7b1b41e31eb",
     "grade": false,
     "grade_id": "cell-62e9662dcba00ffe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will now use your `leader_board_predict_fn` function for calculating the accuracy of your model. We provide the code for testing your loaded model on the visible training data. We will also evaluate your model's performance on the test dataset (the test dataset should only be used for evaluation and is **NOT** to be used for training your model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eebe07a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49bb0771e72597a654efa8ac01ddc729",
     "grade": true,
     "grade_id": "cell-c8f5ac9b0f137931",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_score():\n",
    "    \"\"\"\n",
    "    Function to compute scores for train and test datasets.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import os\n",
    "    import pwd\n",
    "    import time\n",
    "    import pathlib\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    \n",
    "    ### LEADER BOARD TEST\n",
    "    seed = 200\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Calculate the accuracy on the training dataset\n",
    "    # to check that your `leader_board_predict_fn` function \n",
    "    # works without any error\n",
    "    dataset_score = accuracy(dataset_path=DATASET_PATH)\n",
    "\n",
    "    assert isinstance(dataset_score, float), f\"type of dataset_score is {type(dataset_score)}, but it must be float\"\n",
    "    assert 0.0<=dataset_score<=1.0, f\"Value of dataset_score is {dataset_score}, but it must be between 0.0 and 1.0\"\n",
    "\n",
    "    # This is your accuracy score on the visible training dataset\n",
    "    # This is NOT used for the leaderboard.\n",
    "    print(f\"Accuracy score on training data: {dataset_score}\")\n",
    "\n",
    "    # There is a hidden test that will evaluate your trained model on the hidden test set\n",
    "    # This hidden dataset and the accuracy for this will not be visible to you when you\n",
    "    # validate this notebook. The accuracy score on the hidden dataset will be used\n",
    "    # for calculating your leaderboard score.\n",
    "\n",
    "    seed = 200\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    user_id = pwd.getpwuid( os.getuid() ).pw_name\n",
    "    curtime = time.time()\n",
    "    dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    try:  # \n",
    "        HIDDEN_DATASET_PATH = os.path.expanduser(\"/data/mlproject22-test-data/sign_lang_test\")\n",
    "        hiddendataset_score = accuracy(dataset_path=HIDDEN_DATASET_PATH)\n",
    "\n",
    "        assert isinstance(hiddendataset_score, float), f\"type of dataset_score is {type(dataset_score)}, but it must be float\"\n",
    "        assert 0.0<=hiddendataset_score<=1.0, f\"Value of dataset_score is {dataset_score}, but it must be between 0.0 and 1.0\"\n",
    "\n",
    "        print(f\"Leaderboard score: {hiddendataset_score}\")\n",
    "\n",
    "        score_dict = dict(\n",
    "            score_hidden=hiddendataset_score,\n",
    "            score_train=dataset_score,\n",
    "            unixtime=curtime,\n",
    "            user=user_id,\n",
    "            dt=dt_now,\n",
    "            comment=\"\",\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        err = str(e)\n",
    "        score_dict = dict(\n",
    "            score_hidden=float(\"nan\"),\n",
    "            score_train=dataset_score,\n",
    "            unixtime=curtime,\n",
    "            user=user_id,\n",
    "            dt=dt_now,\n",
    "            comment=err\n",
    "        )\n",
    "\n",
    "\n",
    "    #if list(pathlib.Path(os.getcwd()).parents)[0].name == 'source':\n",
    "    #    print(\"we are in the source directory... replacing values.\")\n",
    "    #    print(pd.DataFrame([score_dict]))\n",
    "    #    score_dict[\"score_hidden\"] = -1\n",
    "    #    score_dict[\"score_train\"] = -1\n",
    "    #    print(\"new values:\")\n",
    "    #    print(pd.DataFrame([score_dict]))\n",
    "\n",
    "    pd.DataFrame([score_dict]).to_csv(\"sign_lang.csv\", index=False)\n",
    "\n",
    "    ### LEADER BOARD TEST\n",
    "    \n",
    "get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b32df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
